{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hBTmF6nzyh8"
      },
      "source": [
        "### Connecting to a runtime\n",
        "\n",
        "Before you begin, connect to a Python GPU-enabled runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu-r4pAs6Pm6"
      },
      "source": [
        "#### Install required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6x_jWMWaZ4m"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y iputils-ping\n",
        "!apt-get install -y traceroute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N_XpsGz6FVn"
      },
      "source": [
        "#### Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5sV0zcF6JCi"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import torch\n",
        "import time\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MM7cR2U0jAr"
      },
      "source": [
        "#### Confirm that your runtime is GPU-supported\n",
        "\n",
        "Observe the GPU type, its power draw and its maximum power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCya8uYs1Ft4"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6wxxrla1JX-"
      },
      "source": [
        "Next, get the CPU information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km8-9R8Z1iDv"
      },
      "outputs": [],
      "source": [
        "!lscpu |grep 'Model name'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmz_tca72io0"
      },
      "source": [
        "Search for the power range based on the CPU type. We will use this information later for estimating power consumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NYx5dVm-Vwp"
      },
      "source": [
        "### 1. GPU cold start energy consumption\n",
        "\n",
        "Cloud GPU nodes may be \"sleeping\". Powering them up wastes energy and time. Local GPUs don't have this issue.\n",
        "\n",
        "Run the following code and observe the difference in response time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eL4YCfh-yjN"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "torch.cuda.is_available()\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"Initialization time: {t1-t0:.5f} s\")\n",
        "\n",
        "t0 = time.time()\n",
        "torch.cuda.is_available()\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"Second run time: {t1-t0:.5f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZsS6_7nW3oQ"
      },
      "source": [
        "### 2. CPU vs. GPU performance\n",
        "\n",
        "However, utilizing GPU can significantly reduce execution time. Here, we will compare CPU and GPU performance on a matrix multiplication task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZGH8peJXGRS"
      },
      "outputs": [],
      "source": [
        "for device in [\"cpu\", \"cuda\"]:\n",
        "    if device == \"cuda\" and not torch.cuda.is_available():\n",
        "        print(\"GPU unavailable, skipping.\")\n",
        "        continue\n",
        "\n",
        "    device = torch.device(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    x = torch.rand(5000, 5000, device=device)\n",
        "\n",
        "    t0 = time.time()\n",
        "    for i in range(10):\n",
        "      x = x @ x\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(f\"Computation time: {t1-t0:.5f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMXk4b571CSc"
      },
      "source": [
        "What is the difference in performance? Write down your observations.\n",
        "\n",
        "Based on the models of CPU and GPU used, find information about their power draw and estimate the energy consumption in kWh."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn7Pn5QMXm80"
      },
      "source": [
        "### 3. Colab cloud VM location\n",
        "\n",
        "Running processes on a cloud instance using a GPU can significantly decerease the computation time. However, the carbon emissions depend largely on the geolocation of the server where the virtual machine (VM) is running.\n",
        "\n",
        "The geolocation of the Google Cloud VM your notebook is running on can be determined by obtaining its IP address."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N3H6oXSZx_R"
      },
      "source": [
        "Get the external IP address of the Colab server instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75x0wr8pZwgY"
      },
      "outputs": [],
      "source": [
        "!curl -s https://ipinfo.io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q14f7vP-WlOy"
      },
      "source": [
        "### 4. Carbon intensity of cloud compute"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYdBXQUuYEZE"
      },
      "source": [
        "First, print again the location of the Colab VM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8zeRg5lOvEz"
      },
      "outputs": [],
      "source": [
        "loc = requests.get(\"https://ipinfo.io\").json()\n",
        "print(f\"Colab server region: {loc[\"country\"], loc[\"region\"]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK50RNWuS1WC"
      },
      "source": [
        "Now, we will investigate the carbon intensity of the region where Colab VM is running. To do so, follow the next steps:\n",
        "\n",
        "1. Open https://app.electricitymaps.com/settings/api-access, copy your Test API Key and paste it below. Please note that for doing so, you will need to register first.\n",
        "\n",
        "2. Go to https://app.electricitymaps.com/map/live/fifteen_minutes and find the exact region your VM is located on.\n",
        "\n",
        "3. Go to https://app.electricitymaps.com/developer-hub/playground and search for it under Region dropdown menu. After selection, it will appear as a zone paramerer in the Request string. Paste it in the url variable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh-M8hw-RrnI"
      },
      "outputs": [],
      "source": [
        "token = f\"oIQdVkbRw40zntgnx4J9\"\n",
        "region = loc[\"country\"]\n",
        "\n",
        "url = f\"https://api.electricitymaps.com/v3/carbon-intensity/latest?zone=US-NW-PACW&temporalGranularity=hourly&emissionFactorType=direct\"\n",
        "data = requests.get(url, headers={\"auth-token\": token}).json()\n",
        "\n",
        "print(f\"Carbon intensity for a Colab server in {loc[\"country\"]} is: {data['carbonIntensity']} g CO2e/kWh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFnLzWHVT9Z"
      },
      "source": [
        "On the https://app.electricitymaps.com/map/live/fifteen_minutes and investigate from which sources the energy comes from and which sources are dominant. Compare it to your country's carbon intensity. Write down your observations.\n",
        "\n",
        "For the previous matrix multiplication example, now calculate the carbon emissions in g CO2e."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSsINjUz4O3B"
      },
      "source": [
        "### 5. Data transfer speed\n",
        "\n",
        "Cloud computing that ocurrs thousands of kilometers away is also increasing latency and energy for data transfer.\n",
        "\n",
        "Therefore, next we are going to measure the data transfer latency and energy consumtion based on the location the data is hosted on.\n",
        "\n",
        "For measuring data transfer, Colab cannot sniff packets, but we *can* measure effective throughput."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP_0C6IMIYe1"
      },
      "source": [
        "#### 4.1. Download the MNIST dataset to your computer\n",
        "\n",
        "Open terminal and type\n",
        "\n",
        "```\n",
        "wget https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
        "```\n",
        "\n",
        "The output will give you the length of the file in bytes and the time it takes to transfer the file from a remote server to your local machine. Write it down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fABGEPo-4ikJ"
      },
      "source": [
        "#### 4.2 Upload MNIST from your computer to Colab\n",
        "\n",
        "Now, upload mnist.npz file from your local machine to Colab and time it.\n",
        "\n",
        "`files.upload()` prompts the user to upload files from their local machine to the rutnime. It returns a dictionary of the files which were uploaded. The dictionary is keyed by the file name and values are the data which were uploaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrTL6Tmd1ZgI"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "uploaded = files.upload()\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"Local->Colab upload time: {t1-t0:.2f} s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kMXzdSg60bh"
      },
      "source": [
        "#### 4.3. Download the same file from Colab back to your machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgTt8cNF5yqW"
      },
      "outputs": [],
      "source": [
        "files.download('/content/mnist.npz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0nk3daa-Hhr"
      },
      "source": [
        "Download time cannot be measured, but can you observe a difference between downloading the same file from its original location using `wget` and from Colab?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL_ojqCwBZwU"
      },
      "source": [
        "4.4. Download MNIST directly to Colab\n",
        "\n",
        "Now, download the same dataset to Colab using `wget`.\n",
        "How much time this takes and what is the difference when the file is uploaded from your local machine and from another remote cloud storage?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvVvfBDF-OSr"
      },
      "outputs": [],
      "source": [
        "!wget -O /dev/null https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb52G8HRCXBc"
      },
      "source": [
        "The output will give you the length of the file in bytes and the time it takes to transfer the file from a remote cloud storage to Colab.\n",
        "\n",
        "If you were to run a machine learning model on Colab, where would you download the data from? Does it depend on the location where the data is stored?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raP9RTPTKzZ_"
      },
      "source": [
        "Now, let's try to estimate the distance to the dataset location.\n",
        "\n",
        "First, let's use ping to roughly estimate the distance from our Google Cloud VM where Colab is running to the location of the Google Cloud Storage where our MNIST dataset is locates.\n",
        "\n",
        "CAUTION: Ping will only tell you:\n",
        " Packets transmitted and received.\n",
        "\n",
        "* Round-trip time (RTT)\n",
        "* The IP address that responded\n",
        "* Packet loss percentage\n",
        "* Minimum, maximum, average, and standard deviation of the round-trip time\n",
        "\n",
        "The IP address you get is not reliable for geolocation because Google Cloud Storage endpoints (including storage.googleapis.com) are served using anycast routing. This means that many servers around the world share the same IP address, your packets are routed to the nearest Google edge location, and the IP does not indicate the physical data center.\n",
        "\n",
        "However, ping can help you roughly estimate proximity:\n",
        "\n",
        "* RTT ~1–10 ms - likely the same region\n",
        "* RTT ~30–80 ms - likely the same continent\n",
        "* RTT >150 ms - probably cross-continent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHIuVVm-Caaj"
      },
      "outputs": [],
      "source": [
        "!ping -c 5 www.storage.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN9qCLQUM6Pv"
      },
      "source": [
        "What can you conclude based on the round-trip time?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO9FRdCDOC5O"
      },
      "source": [
        "Let's see how many hops exist between Google Cloud VM and Google Cloud Storage where MNIST is stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab2GrdU6Ct4y"
      },
      "outputs": [],
      "source": [
        "!traceroute www.storage.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5WCnSY8OjQ5"
      },
      "source": [
        "Run the same command on your local machine. The last hop is the hop to the destination (Google Cloud Storage).\n",
        "\n",
        "What latencies do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juTGSuaG80XD"
      },
      "source": [
        "### 6. ML model training energy consumption\n",
        "\n",
        "Finally, we will investigate the energy consumed when training a ML model on CPU and GPU.\n",
        "\n",
        "We will measure the execution time and the CPU and GPU utilization and power consumption in order to estimate the carbon emissions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GQw2dED-lW7"
      },
      "source": [
        "Install the necessary dependencies first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsipFmT0-nj4"
      },
      "outputs": [],
      "source": [
        "!pip install psutil\n",
        "!pip install gcsfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXdm_di5-tDY"
      },
      "source": [
        "Load the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP1T_GMk-oS7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import gcsfs\n",
        "import gc\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "import psutil\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "\n",
        "import time\n",
        "from time import perf_counter as timer\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4344iHI-0ro"
      },
      "source": [
        "Resource logger\n",
        "\n",
        "Examine the class and describe what it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hmPO6-_-vop"
      },
      "outputs": [],
      "source": [
        "class ResourceLogger:\n",
        "    def __init__(self, interval=0.2):\n",
        "        self.interval = interval\n",
        "        self.running = False\n",
        "        self.data = []\n",
        "\n",
        "    def get_gpu_stats(self):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"nvidia-smi\",\n",
        "                 \"--query-gpu=utilization.gpu,utilization.memory,power.draw\",\n",
        "                 \"--format=csv,noheader,nounits\"],\n",
        "                stdout=subprocess.PIPE, text=True\n",
        "            ).stdout.strip()\n",
        "\n",
        "            gpu_util, mem_util, power = map(float, re.split(r\",\\s*\", result))\n",
        "            return gpu_util, mem_util, power\n",
        "        except:\n",
        "            return None, None, None\n",
        "\n",
        "    def sample(self):\n",
        "        while self.running:\n",
        "            cpu = psutil.cpu_percent(interval=None)\n",
        "            gpu_util, mem_util, power = self.get_gpu_stats()\n",
        "            t = time.time()\n",
        "\n",
        "            self.data.append({\n",
        "                \"timestamp\": t,\n",
        "                \"cpu_util\": cpu,\n",
        "                \"gpu_util\": gpu_util,\n",
        "                \"gpu_mem\": mem_util,\n",
        "                \"gpu_power_w\": power\n",
        "            })\n",
        "            time.sleep(self.interval)\n",
        "\n",
        "    def start(self):\n",
        "        self.running = True\n",
        "        self.thread = threading.Thread(target=self.sample)\n",
        "        self.thread.start()\n",
        "\n",
        "    def stop(self):\n",
        "        self.running = False\n",
        "        self.thread.join()\n",
        "\n",
        "logger = ResourceLogger(interval=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8OCMorI_Aa4"
      },
      "source": [
        "Download and load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH27hqq8_OEp"
      },
      "outputs": [],
      "source": [
        "!wget -O YearPredictionMSD.txt.zip \"https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "765blpow_Eqs"
      },
      "outputs": [],
      "source": [
        "local_url = \"YearPredictionMSD.txt.zip\"\n",
        "\n",
        "print(\"Loading CSV...\")\n",
        "start = timer()\n",
        "year = pd.read_csv(local_url, header=None)\n",
        "\n",
        "load_time = timer() - start\n",
        "print(f\"Dataset load time: {load_time}s\")\n",
        "\n",
        "x = year.iloc[:, 1:].to_numpy(dtype=np.float32)\n",
        "y = year.iloc[:, 0].to_numpy(dtype=np.float32)\n",
        "\n",
        "del year\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where do you think this dataset is located and why do you think it takes this amount of time to load it?"
      ],
      "metadata": {
        "id": "g6zY6QtBHQtS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cExwvxR5AGBJ"
      },
      "source": [
        "Preprocess and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoE_HH8vAGk9"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0)\n",
        "\n",
        "# Normalization\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "scaler_x.fit(x_train)\n",
        "x_train = scaler_x.transform(x_train)\n",
        "x_test = scaler_x.transform(x_test)\n",
        "\n",
        "scaler_y.fit(y_train.reshape(-1, 1))\n",
        "y_train = scaler_y.transform(y_train.reshape(-1, 1)).ravel()\n",
        "y_test = scaler_y.transform(y_test.reshape(-1, 1)).ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3jHi0x2ARSC"
      },
      "source": [
        "PyTorch Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKHv5pabAHHW"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, 1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gflDp4zxAWLp"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9VOixG8AWdO"
      },
      "outputs": [],
      "source": [
        "def train_lbfgs(X, y, device):\n",
        "    X = torch.tensor(X, dtype=torch.float16, device=device)\n",
        "    y = torch.tensor(y, dtype=torch.float16, device=device)\n",
        "\n",
        "    model = LinearRegressionModel(X.shape[1]).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.LBFGS(model.parameters(), lr=0.5, max_iter=200, history_size=5)\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, y)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(closure)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrgg-DLjCFmI"
      },
      "source": [
        "Helper scripts for energy use calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt4AxU3DCIwO"
      },
      "outputs": [],
      "source": [
        "def gpu_power_to_energy(logs, power_key=\"gpu_power_w\"):\n",
        "    \"\"\"\n",
        "    logs: list of dicts with 'timestamp' and power_key\n",
        "    Integrate power over time using piecewise-constant on logs intervals\n",
        "    Returns energy_kwh, duration_s\n",
        "    \"\"\"\n",
        "    if len(logs) < 2:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    energy_j = 0.0\n",
        "    for i in range(1, len(logs)):\n",
        "        t0 = logs[i-1][\"timestamp\"]\n",
        "        t1 = logs[i][\"timestamp\"]\n",
        "        dt = max(0.0, t1 - t0)\n",
        "        p0 = logs[i-1].get(power_key)\n",
        "        p1 = logs[i].get(power_key)\n",
        "        # Integration approximation (trapezoidal integration)\n",
        "        energy_j += 0.5 * (p0 + p1) * dt  # watts * seconds = joules\n",
        "\n",
        "    duration = logs[-1][\"timestamp\"] - logs[0][\"timestamp\"]\n",
        "    energy_kwh = energy_j / 3_600_000.0\n",
        "    return energy_kwh, duration\n",
        "\n",
        "def cpu_util_to_energy(logs, cpu_power_w):\n",
        "    \"\"\"\n",
        "    logs: list of dicts with 'timestamp' and \"cpu_util\"\n",
        "    Convert sampled CPU percent series to estimated energy assuming a host cpu_power_w\n",
        "    Returns energy_kwh, duration_s\n",
        "    \"\"\"\n",
        "    if len(logs) < 2:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    total_area = 0.0\n",
        "    total_time = 0.0\n",
        "    for i in range(1, len(logs)):\n",
        "        t0 = logs[i-1][\"timestamp\"]\n",
        "        t1 = logs[i][\"timestamp\"]\n",
        "        dt = max(0.0, t1 - t0)\n",
        "        p0 = logs[i-1].get(\"cpu_util\") or 0.0\n",
        "        p1 = logs[i].get(\"cpu_util\") or 0.0\n",
        "        # Integration approximation (trapezoidal integration)\n",
        "        total_area += 0.5 * (p0 + p1) * dt\n",
        "        total_time += dt\n",
        "\n",
        "    avg_percent = (total_area / total_time) if total_time>0 else 0.0\n",
        "    cpu_energy_j = cpu_power_w * (avg_percent/100.0) * total_time\n",
        "    cpu_energy_kwh = cpu_energy_j / 3_600_000.0\n",
        "\n",
        "    return cpu_energy_kwh, total_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs4zd81dAfOa"
      },
      "source": [
        "Run training on CPU and GPU with utilization logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sirzyBJKFNNm"
      },
      "outputs": [],
      "source": [
        "# Estimate CPU power based on CPU model\n",
        "cpu_power_w = 70.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFqs1EfHAav8"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "for device in [\"cpu\", \"cuda\"]:\n",
        "    if device == \"cuda\" and not torch.cuda.is_available():\n",
        "        print(\"GPU unavailable, skipping.\")\n",
        "        continue\n",
        "    print(f\"Running on {device}...\")\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
        "    Y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    X_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
        "    Y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, Y_train)\n",
        "    test_dataset = TensorDataset(X_test, Y_test)\n",
        "\n",
        "    model = LinearRegressionModel(X_train.shape[1]).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Training loop\n",
        "    loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
        "\n",
        "    logger.data = []\n",
        "    logger.start()\n",
        "\n",
        "    start = timer()\n",
        "    epochs = 10\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    train_time = timer() - start\n",
        "\n",
        "    logger.stop()\n",
        "\n",
        "    # Model testing\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X_test)\n",
        "        mse = metrics.mean_squared_error(\n",
        "            Y_test.cpu().numpy(), preds.cpu().numpy()\n",
        "        )\n",
        "\n",
        "    print(f\"PyTorch {device} MSE: {mse} (training time: {train_time}s)\")\n",
        "    cpu_energy_kw, cpu_duration = cpu_util_to_energy(logger.data, cpu_power_w)\n",
        "    gpu_energy_kw, gpu_duration = gpu_power_to_energy(logger.data, \"gpu_power_w\")\n",
        "    print(f\"CPU energy use: {cpu_energy_kw} kWh\")\n",
        "    print(f\"GPU energy use: {gpu_energy_kw} kWh\")\n",
        "\n",
        "    del model, X_train, Y_train, X_test, Y_test\n",
        "    del train_dataset, test_dataset, loader\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the results and comment on them. Use the energy usage information to calculate carbon emissions based on the previously determined VM location."
      ],
      "metadata": {
        "id": "QtMcUyAMILwS"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}